from typing import Optional
from pydantic import Field
from autotrain.trainers.common import AutoTrainParams


class AudioClassificationGaudiParams(AutoTrainParams):
  model_name_or_path: str = Field("facebook/wav2vec2-base", Title="Model name or path")
  feature_extractor_name: str = Field("null", Title="Feature extractor name")
  freeze_feature_encoder: bool = Field(False, Title="Freeze feature encoder")
  dataset_config_name: str = Field("ks", Title="Dataset config name")
  do_train: bool = Field(True, Title="Do train")
  train_split_name: str = Field("train", Title="Train split name")
  eval_split_name: str = Field("test", Title="Eval split name")
  gaudi_config_name: str = Field("Habana/wav2vec2", Title="Gaudi config name")
  audio_column_name: str = Field("audio", Title="Audio column name")
  label_column_name: str = Field("label", Title="Label column name")
  max_length_seconds: int = Field(1, Title="Max length seconds")
  num_train_epochs: int = Field(50, Title="Number of training epochs")
  seed: int = Field(42, Title="Seed")
  attention_mask: bool = Field(True, Title="Attention mask")
  per_device_train_batch_size: int = Field(64, Title="Per device training batch size")
  learning_rate: float = Field(2e-5, Title="Learning rate")
  warmup_ratio: float = Field(0.1, Title="Warmup ratio")
  optimizer: str = Field("adamw_torch_fused", Title="Optimizer")
  lr_scheduler_type: str = Field("reduce_lr_on_plateau", Title="Learning rate scheduler type")
  gradient_accumulation_steps: int = Field(1, Title="Gradient accumulation steps")
  mixed_precision: str = Field("fp32", Title="Mixed precision")
  throughput_warmup_steps: int = Field(3, Title="Throughput warmup steps")
  dataloader_num_workers: int = Field(4, Title="Dataloader num workers")
  use_habana: bool = Field(True, Title="Use Habana device")
  use_lazy_mode: bool = Field(True, Title="Use lazy mode")
  overwrite_output_dir: bool = Field(True, Title="Overwrite output directory")
  remove_unused_columns: bool = Field(True, Title="Remove unused columns")
  use_hpu_graphs: bool = Field(True, Title="Use HPU graphs")
  use_hpu_graphs_for_training: bool = Field(True, Title="Use HPU graphs for training")
  use_hpu_graphs_for_inference: bool = Field(True, Title="Use HPU graphs for inference")
  non_blocking_data_copy: bool = Field(False, Title="Non-blocking data copy")
  evaluation_strategy: str = Field("epoch", Title="Evaluation strategy")
  backend: str = Field("local", Title="Backend")
  dataset_name: str = Field("stanfordnlp/imdb", Title="Dataset name")
  username: Optional[str] = Field(None, title="Hugging Face Username")
  token: Optional[str] = Field(None, title="Hub Token")
  output_dir: str = Field("output", Title="Output directory")
  push_to_hub: bool = Field(False, title="Push to hub")
  project_name: str = Field("project-name", Title="Output directory")
  