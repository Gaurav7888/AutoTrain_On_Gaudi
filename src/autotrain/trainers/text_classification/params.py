from typing import Optional

from pydantic import Field

from autotrain.trainers.common import AutoTrainParams


class TextClassificationParams(AutoTrainParams):
    model_name_or_path: str = Field("google-bert/bert-base-uncased", Title="Model name or path")
    backend: str = Field("local", Title="Backend")
    dataset_name: str = Field("stanfordnlp/imdb", Title="Dataset name")
    train_split: str = Field("train", Title="Train split")
    valid_split: str = Field("test", Title="Validation split")
    column_mapping_text_column: str = Field("text", Title="Column mapping text column")
    column_mapping_target_column: str = Field("label", Title="Column mapping target column")
    max_seq_length: int = Field(512, Title="Max sequence length")
    num_train_epochs: int = Field(4, Title="Number of training epochs")
    per_device_train_batch_size: int = Field(1, Title="Per device training batch size")
    learning_rate: float = Field(2e-5, Title="Learning rate")
    optim: str = Field("adamw_torch_fused", Title="Optimizer")
    lr_scheduler_type: str = Field("reduce_lr_on_plateau", Title="Learning rate scheduler type")
    gradient_accumulation_steps: int = Field(1, Title="Gradient accumulation steps")
    mixed_precision: str = Field("fp32", Title="Mixed precision")
    use_habana: bool = Field(True, Title="Use Habana device")
    use_hpu_graphs: bool = Field(True, Title="Use HPU graphs")
    use_hpu_graphs_for_training: bool = Field(True, Title="Use HPU graphs for training")
    use_hpu_graphs_for_inference: bool = Field(True, Title="Use HPU graphs for inference")
    non_blocking_data_copy: bool = Field(False, Title="Non-blocking data copy")
    evaluation_strategy: str = Field("epoch", Title="Evaluation strategy")
    username: Optional[str] = Field(None, title="Hugging Face Username")
    token: Optional[str] = Field(None, title="Hub Token")
    output_dir: str = Field("output", Title="Output directory")
    project_name: str = Field("project-name", title="Output directory")
    push_to_hub: bool = Field(False, title="Push to hub")
    data_path: str = Field("stanfordnlp/imdb", title="Data path")
    model: str = Field("bert-base-uncased", title="Model name")
    lr: float = Field(5e-5, title="Learning rate")
    epochs: int = Field(3, title="Number of training epochs")
    max_seq_length: int = Field(128, title="Max sequence length")
    batch_size: int = Field(8, title="Training batch size")
    warmup_ratio: float = Field(0.1, title="Warmup proportion")
    gradient_accumulation: int = Field(1, title="Gradient accumulation steps")
    optimizer: str = Field("adamw_torch", title="Optimizer")
    scheduler: str = Field("linear", title="Scheduler")
    weight_decay: float = Field(0.0, title="Weight decay")
    max_grad_norm: float = Field(1.0, title="Max gradient norm")
    seed: int = Field(42, title="Seed")
    train_split: str = Field("train", title="Train split")
    valid_split: Optional[str] = Field(None, title="Validation split")
    text_column: str = Field("text", title="Text column")
    target_column: str = Field("label", title="Target column")
    logging_steps: int = Field(-1, title="Logging steps")
    auto_find_batch_size: bool = Field(False, title="Auto find batch size")
    mixed_precision: Optional[str] = Field(None, title="fp16, bf16, or None")
    save_total_limit: int = Field(1, title="Save total limit")
    token: Optional[str] = Field(None, title="Hub Token")
    eval_strategy: str = Field("epoch", title="Evaluation strategy")
    username: Optional[str] = Field(None, title="Hugging Face Username")
    log: str = Field("none", title="Logging using experiment tracking")
    early_stopping_patience: int = Field(5, title="Early stopping patience")
    early_stopping_threshold: float = Field(0.01, title="Early stopping threshold")